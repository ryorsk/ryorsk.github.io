<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Ryosuke Araki</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-112919622-1"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'UA-112919622-1');
		</script>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<div id="header">

				<div class="top">

					<!-- Logo -->
						<div id="logo">
							<span class="image avatar48"><img src="images/avatar.jpg" alt="" /></span>
							<h1 id="title">Ryosuke Araki</h1>
							<p>荒木 諒介</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="#top" id="top-link"><span class="icon solid fa-home">Top</span></a></li>
								<li><a href="#bio" id="bio-link"><span class="icon solid fa-user">略歴</span></a></li>
								<li><a href="#research" id="research-link"><span class="icon solid fa-brain">研究内容</span></a></li>
								<li><a href="#pub" id="bub-link"><span class="icon solid fa-newspaper">業績</span></a></li>
								<li><a href="#award" id="award-link"><span class="icon solid fa-award">受賞</span></a></li>
								<li><a href="#ext" id="ext-link"><span class="icon solid fa-chalkboard-teacher">学外活動等</span></a></li>
								<li><a href="#skill" id="skill-link"><span class="icon solid fa-th">所持資格・スキル</span></a></li>

							</ul>
						</nav>

				</div>

				<div class="bottom">

					<!-- Social Icons -->
						<ul class="icons">
							<li><a href="https://ryorsk.wordpress.com/" class="icon brands fa-wordpress"><span class="label">WordPress(Blog)</span></a></li>
							<li><a href="https://twitter.com/ryors_k" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
							<li><a href="https://www.facebook.com/AiRfaith" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
							<li><a href="https://github.com/ryorsk" class="icon brands fa-github"><span class="label">Github</span></a></li>
							<li><a href="https://www.linkedin.com/in/ryosuke-araki-b79660156/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://www.slideshare.net/RyosukeAraki" class="icon brands fa-slideshare"><span class="label">SlideShare</span></a></li>
							<li><a href="http://www.mprg.cs.chubu.ac.jp/~ryorsk/Research/" class="icon solid fa-warehouse"><span class="label">ResearchPage(members only)</span></a></li>
						</ul>

				</div>

			</div>

		<!-- Main -->
			<div id="main">

				<!-- Top -->
					<section id="top" class="one dark cover">
						<div class="container">

							<header>
								<h2 class="alt">荒木 諒介 (Ryosuke Araki) </h2>
								<p>
									中部大学大学院 工学研究科 博士後期課程 情報工学専攻<br>
									機械知覚&ロボティクスグループ（MPRG: Machine Perception and Robotics Group）<br>
									<a href="http://xpaperchallenge.org/cv/">cvpaper.challenge</a> 研究メンバ<br>
									<a href="https://nagoyacv.connpass.com">名古屋CV・PRML勉強会</a>5代目幹事<br>
									独立行政法人 日本学術振興会 特別研究員（DC2）<br>
									e-mail: ryorsk |at| mprg.cs.chubu.ac.jp
								</p>
							</header>

							<div class="news">
								<span class="news_title">News</span>
								<ul>
									<li>2020/03/03 <a href="https://www.jdla.org/">（一社）JDLA</a>の主催するディープラーニング検定のE資格に合格しました．</li>
									<li>2020/01/22 ICRA2020に論文が採択されました．</li>
									<li>2019/12/30 日本学術振興会特別研究員（DC2）に採用内定となりました．</li>
									<li>2019/10/05 Webページをリニューアルしました．</li>
								</ul>
							</div>

						</div>
					</section>

				<!-- 略歴 -->
					<section id="bio" class="two">
						<div class="container">

							<header>
								<h2>略歴</h2>
							</header>

							<a href="#" class="image featured"><img src="images/banner3.jpg" alt="" /></a>

							<ul>
								<li>2013年3月 愛知県立岩倉総合高等学校 総合学科 卒業．</li>
								<li>2013年4月 中部大学 工学部 情報工学科 入学．</li>
								<li>2017年3月 中部大学 工学部 情報工学科 次席卒業．学士（工学）．（副専攻 コミュニケーション学 修了）</li>
								<li>2017年4月 中部大学大学院 工学研究科 博士前期課程 情報工学専攻 入学．</li>
								<li>2019年3月 中部大学大学院 工学研究科 博士前期課程 情報工学専攻 修了．修士（工学）．</li>
								<li>2019年4月 中部大学大学院 工学研究科 博士後期課程 情報工学専攻 入学．</li>
								<li>2019年5月 独立行政法人 日本学術振興会 特別研究員（DC2）．</li>
							</ul>

						</div>
					</section>


				<!-- 研究内容 -->
					<section id="research" class="three">
						<div class="container">

							<header>
								<h2>研究内容</h2>
							</header>

							<p>
								深層ニューラルネットワーク (DNN) や深層畳み込みニューラルネットワーク (DCNN) などの深層学習はコンピュータビジョン分野において高い認識精度を達成し，
								今日の第3次AIブームを牽引しています．同技術は，ロボティクス分野でも物体の把持位置検出やロボット動作の自動獲得など，普遍的手法として確立されつつあります．
								所属する研究グループ「MPRG」では，深層学習を活用したロボットの制御に必要な画像認識技術の研究や，画像認識結果を与えてロボットを制御する研究に取り組んでいます．
								特に，私は「ロボットでの物体把持位置検出」「把持対象物体の認識」に焦点を当てて研究しています．
							</p>

							<p>（各研究はクリックすると詳細が表示されます．）</a>

							<div class="row">
								<div class="col-4 col-12-mobile">
									<article class="item">
										<a href="" class="image fit js-modal-open" data-target="graspability"><img src="images/grasping_det.jpg" alt="" /></a>
										<header>
											<h3><a href="" class="js-modal-open" data-target="graspability">Graspabilityを導入したDCNNによる物体把持位置検出</a></h3>
										</header>
									</article>
									<article class="item">
										<a href="" class="image fit js-modal-open" data-target="objectness"><img src="images/objectness-ssd.jpg" alt="" /></a>
										<header>
											<h3><a href="" class="js-modal-open" data-target="objectness">Objectnessを導入したSSDによる未学習物体の検出</a></h3>
										</header>
									</article>
								</div>
								<div class="col-4 col-12-mobile">
									<article class="item">
										<a href="" class="image fit js-modal-open" data-target="mt_dssd"><img src="images/mt-dssd.jpg" alt="" /></a>
										<header>
											<h3><a href="" class="js-modal-open" data-target="mt_dssd">MT-DSSDを用いた物体と物体把持位置の同時検出</a></h3>
										</header>
									</article>
									<article class="item">
										<a href="" class="image fit js-modal-open" data-target="occlusion_det"><img src="images/occlusion_det.jpg" alt="" /></a>
										<header>
											<h3><a href="" class="js-modal-open" data-target="occlusion_det">[共同研究] 多品種ばら積みピッキングにおける物体間の上下関係の予測</a></h3>
										</header>
									</article>
								</div>
								<div class="col-4 col-12-mobile">
									<article class="item">
										<a href="" class="image fit js-modal-open" data-target="gan_da"><img src="images/gan-da.jpg" alt="" /></a>
										<header>
											<h3><a href="" class="js-modal-open" data-target="gan_da">[共同研究] 脳腫瘍検出に向けたPGGANによるMR画像の水増し</a></h3>
										</header>
									</article>
								</div>
							</div>

						</div>
					</section>

				<!-- 業績 -->
					<section id="pub" class="four">
						<div class="container">

							<header>
								<h2>業績</h2>
							</header>

							<h3>学位論文</h3>
							<ul>
								<li>2019年2月 修士論文 “<a href="http://mprg.jp/data/FLABResearchArchive/Master/M18/Abstract/araki.pdf">深層学習を用いた物体認識と物体把持位置検出に関する研究</a>”</li>
								<li>2017年2月 卒業論文 “<a href="http://mprg.jp/data/FLABResearchArchive/Bachelor/B16/Abstract/araki.pdf">Graspabilityを導入したDCNNによる物体把持位置検出</a>”</li>
							</ul>

							<h3>Journal / Book chapter</h3>
							<ul>
								<li>C. Han, L. Rundo, <b>R. Araki</b>, Y. Furukawa, G. Mauri, H. Nakayama and H. Hayashi,<br>
									“<a href="https://ieeexplore.ieee.org/document/8869751">Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image Augmentation for Tumor Detection</a>”,<br>
									IEEE Access, doi: 10.1109/ACCESS.2019.2947606, IEEE, 2019. (査読あり, IF: 4.098)</li>

								<li>C. Han, L. Rundo, <b>R. Araki</b>, Y. Furukawa, G. Mauri, H. Nakayama and H. Hayashi,<br>
									“<a href="https://link.springer.com/chapter/10.1007/978-981-13-8950-4_27">Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor Detection</a>”,<br>
									R. J. Howlett, and L. C. Jain (eds.) Smart Innovation, System and Technologies, vol. 151, pp. 291-303, Springer, 2020. (査読あり，Book chapter)</li>

								<li><b>荒木 諒介</b>, 長谷川 昂宏, 山内 悠嗣, 山下 隆義, 藤吉 弘亘, 堂前 幸康, 川西 亮輔, 関 真規人,<br>
									“<a href="https://ci.nii.ac.jp/naid/130007511508">Graspabilityを導入したDCNNによる物体把持位置検出</a>”,<br>
									日本ロボット学会誌, 36巻, 8号, pp. 41-48, 日本ロボット学会, 2018.（査読あり，論文誌）</li>
							</ul>

							<h3>Presentation (Domestic / International / Workshop etc.)</h3>
							<ul>
								<li><b>荒木 諒介</b>, 大西 剛史, 平野 正徳, 真野 航輔, 平川 翼, 山下 隆義, 藤吉 弘亘,<br>
									“画像生成ネットワークの逆伝播に基づく繰り返し処理による物体の6D姿勢推定”,<br>
									画像の認識・理解シンポジウム（MIRU）, 2020. （査読あり，ショートオーラル発表）</li>

								<li><b>R. Araki</b>, T. Onishi, T. Hirakawa, T. Yamashita, H. Fujiyoshi,<br>
									“MT-DSSD: Deconvolutional Single Shot Detector Using Multi Task Learning for Object Detection, Segmentation, and Grasping Detection”,<br>
									IEEE International Conference on Robotics and Automation (ICRA), 2020. (査読あり)</li>

								<li>Y. Inagaki, <b>R. Araki</b>, T. Yamashita, H. Fujiyoshi,<br>
									“<a href="http://mprg.jp/publications/c20191106_inagaki">Detecting Layered Structures of Partially Occluded Objects for Bin Picking</a>”,<br>
									IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019. (査読あり，oral)</li>

								<li>稲垣 雄介, <b>荒木 諒介</b>, 平川 翼, 山下 隆義, 藤吉 弘亘,<br>
									“<a href="http://mprg.jp/publications/f20190904_inagaki">多品種ばら積みピッキングにおける物体間の上下関係の予測とデータセットの提案</a>”,<br>
									日本ロボット学会学術講演会（RSJ）, 2019.（口頭発表）</li>

								<li><b>荒木 諒介</b>, 大西 剛史, 平川 翼, 山下 隆義, 藤吉 弘亘,<br>
									“<a href="http://mprg.jp/publications/f20190904_araki">Multi-task DSSDによる物体位置と物体把持位置の同時推定</a>”,<br>
									日本ロボット学会学術講演会（RSJ）, 2019.（口頭発表）</li>

								<li>M. Fujita, Y. Domae, R. Kawanishi, G. Garcia, K. Kato, K Shiratsuchi, R. Haraguchi, <b>R. Araki</b>, H. Fujiyoshi, S. Akizuki, M. Hashimoto, A. Causo, A. Noda, H. Okuda, T. Ogasawara, <br>
									“<a href="https://ieeexplore.ieee.org/document/8842977">Bin-Picking Robot Using a Multi-Gripper Switching Strategy Based on Object Sparseness</a>”,<br>
									IEEE International Conference on Automation Science and Engineering (CASE), 2019. (査読あり，Regular paper, oral)</li>

								<li>韓 昌熙, 早志 英朗, ルンド レオナルド, <b>荒木 諒介</b>, 永野 雄大, 古川 悠次郎, マウリ ジャンカルロ, 中山 英樹,<br>
									“［ショートペーパー］限られた医用画像で高い精度を出す 〜 脳腫瘍検出に向けた、PGGANによるMR画像の水増し 〜”,<br>
									医用画像研究会（MI）, 2019. （査読あり，口頭発表）</li>

								<li><b>荒木 諒介</b>, 中瀬 架, 福井 宏, 山下 隆義, 藤吉 弘亘,<br>
									“マルチタスク学習を導入したDeconvolutional Single Shot Detectorによる物体検出とセグメンテーションの高精度化”,<br>
									画像の認識・理解シンポジウム（MIRU）, 2018. （査読あり，ポスター発表）</li>

								<li>C. Han, L. Rundo, <b>R. Araki</b>, Y. Furukawa, G. Mauri, H. Nakayama and H. Hayashi,<br>
									“Infinite Brain Tumor Images: Can GAN-based Data Augmentation Improve Tumor Detection on MR Images?”,<br>
									画像の認識・理解シンポジウム（MIRU）, 2018. （査読あり，ポスター発表）</li>

								<li>C. Han, H. Hayashi, L. Rundo, <b>R. Araki</b>, Y. Nagano, Y. Furukawa, G. Mauri and H. Nakayama,<br>
									“Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor Detection”,<br>
									International Computer Vision Summer School (ICVSS), Sicily, Italy, July 2018.</li>

								<li>C. Han, L. Rundo, <b>R. Araki</b>, Y. Furukawa, G. Mauri, H. Nakayama and H. Hayashi,<br>
									“Infinite Brain MR Images: PGGAN-based Data Augmentation for Tumor Detection”,<br>
									The Italian Workshop on Neural Networks (WIRN), 2018. (査読あり，口頭発表)</li>

								<li><b>R. Araki</b>, T. Yamashita and H. Fujiyoshi,<br>
									“ARC2017 RGB-D Dataset for Object Detection and Segmentation”,<br>
									IEEE International Conference on Robotics and Automation (ICRA), 2018. (Late Breaking Poster Presentation)</li>

								<li><b>荒木 諒介</b>, 韓 昌熙, 早志 英朗, Leonardo Rundo,<br>
									“MIRU2017若手プログラム報告 -GANによる合成脳MR画像生成-”,<br>
									情報処理学会 第212回コンピュータビジョンとイメージメディア研究発表会（CVIM）, 2018. （招待講演）</li>

								<li>C. Han, H. Hayashi, L. Rundo, <b>R. Araki</b>, W. Shimoda, S. Muramatsu, Y. Furukawa, G. Mauri and H. Nakayama,<br>
									“<a href="https://ieeexplore.ieee.org/document/8363678/">GAN-based Synthetic Brain MR Image Generation</a>”,<br>
									IEEE International Symposium on Biomedical Imaging (ISBI), 2018.（査読あり，ポスター発表）</li>

								<li><b>荒木 諒介</b>, 長谷川 昂宏, 山内 悠嗣, 山下 隆義, 藤吉 弘亘, 橋本 学, 堂前 幸康,<br>
									“Objectnessを導入したSSDによる学習データに含まれない未知クラスアイテムの検出”,<br>
									動的画像処理実利用化ワークショップ（DIA）, 2018.（ポスター発表）</li>

								<li><b>荒木 諒介</b>, 長谷川 昂宏, 山内 悠嗣, 山下 隆義, 藤吉 弘亘, 橋本 学, 堂前 幸康,<br>
									“Objectnessを導入したSSDによる学習データに含まれない未知クラスアイテムの検出”,<br>
									情報学ワークショップ（WiNF）, 2017.（口頭発表）</li>

								<li>C. Han, <b>R. Araki</b>, W. Shimoda, S. Muramatsu and H. Hayashi,<br>
									“GAN-based Synthetic Medical Image Generation”,<br>
									画像の認識・理解シンポジウム（MIRU）, 2017.（若手プログラム；口頭，ポスター発表）</li>

								<li><b>荒木 諒介</b>, 長谷川 昂宏, 山内 悠嗣, 山下 隆義, 藤吉 弘亘, 堂前 幸康, 川西 亮輔, 関 真規人,<br>
									“把持のしやすさを考慮した物体把持位置検出の高精度化”,<br>
									情報学ワークショップ（WiNF）, 2016.（ポスター発表）</li>

								<li><b>荒木 諒介</b>, 長谷川 昂宏, 山内 悠嗣, 山下 隆義, 藤吉 弘亘, 堂前 幸康, 川西 亮輔, 関 真規人,<br>
									“<a href="http://mprg.jp/publications/f175">Graspabilityを導入したDCNNによる物体把持位置検出</a>”,<br>
									日本ロボット学会学術講演会（RSJ）, 2016.（口頭発表）</li>

								<li>長谷川 昂宏, Xuanyi Sheing, <b>荒木 諒介</b>, 山内 悠嗣, 山下 隆義, 藤吉 弘亘,<br>
									“<a href="http://mprg.jp/publications/f171">Heterogeneous Learningによるオブジェクトネスと物体把持位置の検出</a>”,<br>
									画像センシングシンポジウム（SSII）, 2016.（ポスター発表）</li>
							</ul>

						</div>
					</section>


				<!-- 受賞 -->
					<section id="award" class="five">
						<div class="container">

							<header>
								<h2>受賞</h2>
							</header>

							<h3>個人/1st author</h3>
							<ul>
								<li>2020年 7月 日本学生支援機構 第一種奨学金 特に優れた業績による返還免除（D1, 一部）</li>
								<li>2019年 6月 日本学生支援機構 第一種奨学金 特に優れた業績による返還免除（M1〜M2, 一部）</li>
								<li>2019年 1月 中部大学学長表彰 受賞 [<a href="https://www.chubu.ac.jp/event_reports/detail-400.html">Web</a>]</li>
								<li>2018年 1月 中部大学学長表彰 受賞 [<a href="https://www.chubu.ac.jp/event_reports/detail-377.html">Web</a>]</li>
								<li>2017年 1月 中部大学学長表彰 受賞 [<a href="https://www.chubu.ac.jp/event_reports/detail-354.html">Web</a>]</li>
								<li>2016年11月 第14回情報学ワークショップ (WiNF2016) 優秀論文賞 [<a href="http://www.ist.aichi-pu.ac.jp/winf2016/awards.html">Web</a>]</li>
								<li>2016年 1月 NEXT COMMUNICATION AWARD 2015 サービスアイデア部門 準グランプリ [<a href="https://www.nttdocomo.co.jp/campaign_event/tokai/ncf_award/2015.html">Web</a>]</li>
							</ul>

							<h3>団体</h3>
							<ul>
								<li>2019年 6月 Hack U 中部大学 2019 パーソナルコンピューター研究会賞 (Team: PLR) [<a href="https://hacku.yahoo.co.jp/chubu2019/">Web</a>]</li>
								<li>2018年10月 WRS Future Convenience Store Challenge 陳列・廃棄部門 2位，日本ロボット学会特別賞 (Team: ROC2) [<a href="http://worldrobotsummit.org/wrc2018/">Web</a>]</li>
								<li>2017年12月 WRS Future Convenience Store Challenge トライアル大会 陳列・廃棄部門 1位 (Team: ROC2) [<a href="http://sice-si.org/fcsc/the-technology-division/">Web</a>]</li>
								<li>2017年 8月 MIRU2017 若手プログラム 最優秀研究計画賞 (Group: G)[<a href="http://www.am.sanken.osaka-u.ac.jp/MIRU2017wakate/index.html">Web</a>]</li>
								<li>2017年 7月 Amazon Robotics Challenge 2017 Stow Task 3rd Place (Team: MC^2) [<a href="https://www.amazonrobotics.com/#/roboticschallenge/results">Web</a>]</li>
								<li>2017年 2月 第2回HSRユーザ会 優秀成果賞</li>
							</ul>

						</div>
					</section>


				<!-- 学外活動等 -->
					<section id="ext" class="six">
						<div class="container">

							<header>
								<h2>学外活動・対外的活動等</h2>
							</header>

							<ul>
								<li>2020年 8月 <a href="https://sites.google.com/view/miru2020/若手プログラム">MIRU2020 若手プログラム</a>に参加．第2位． [<a href="https://docs.google.com/presentation/d/1JC4WiEuInAkuSfUwI-CrgWhboKf8AoDpTLRosDmVufk/edit#slide=id.g8d3f695226_1_13">スライド</a>]</li>
								<li>2019年 9月 <a href="https://nagoyacv.connpass.com/event/143388/">第55回名古屋CV・PRML勉強会</a>で公演「MIRU2019参加報告」</li>
								<li>2019年 8月 2019年度中部大学オックスフォード英語短期研修に参加(8/10 - 9/1)．</li>
								<li>2019年 7月 <a href="http://cvim.ipsj.or.jp/MIRU2019/index.php?id=wakate-contents">MIRU2019 若手プログラム</a>に参加．</li>
								<li>2019年 7月 <a href="https://nagoyacv.connpass.com/event/133598/">第54回名古屋CV・PRML勉強会</a>で公演「論文紹介：Triply Supervised Decoder Networks for Joint Detection and Segmentation」
								<li>2019年 4月〜 <a href="https://nagoyacv.connpass.com">名古屋CV・PRML勉強会</a>の5代目幹事に就任．</li>
								<li>2019年 3月 <a href="https://nagoyacv.connpass.com/event/121088/">第53回名古屋CV・PRML勉強会</a>で公演「<a href="https://www.slideshare.net/RyosukeAraki/ss-136687597">楽しい研究のために今からできること 〜新しく研究を始める皆さんへ〜</a>」
								<li>2019年 1月〜 <a href="http://hirokatsukataoka.net/project/cc/index_cvpaperchallenge.html">cvpaper.challenge</a>に参加．</li>
								<li>2018年10月 WRS Future Convenience Store Challenge にチームメンバとして参加．陳列・廃棄部門 2位，日本ロボット学会特別賞受賞．</li>
								<li>2018年 8月 <a href="https://sites.google.com/view/miru2018sapporo/wakate_top">MIRU2018 若手プログラム</a>に参加．</li>
								<li>2018年 6月 <a href="https://nagoyacv.connpass.com/event/87635/">第51回名古屋CV・PRML勉強会</a>で公演「<a href="https://www.slideshare.net/RyosukeAraki/cvpr2018pseudo-mask-augmented-object-detection">CVPR2018論文紹介「Pseudo Mask Augmented Object Detection</a>」
								<li>2018年 5月 <a href="http://hirokatsukataoka.net/project/cc/index_cvpaperchallenge.html">cvpaper.challenge</a>が主宰する<a href="http://hirokatsukataoka.net/project/cc/cvpr2018survey.html">CVPR2018完全読破チャレンジ</a>に参加．</li>
								<li>2018年 4月 <a href="https://nagoyacv.connpass.com/event/82411/">第50回名古屋CV・PRML勉強会</a>で公演「<a href="https://www.slideshare.net/RyosukeAraki/survey-stand-on-the-shoulders-of-giants">Surveyから始まる研究者への道 - Stand on the shoulders of giants -</a>」
								<li>2017年12月 WRS Future Convenience Store Challenge トライアル大会にチームメンバとして参加．陳列・廃棄部門 1位．</li>
								<li>2017年 8月 <a href="http://www.am.sanken.osaka-u.ac.jp/MIRU2017wakate/">MIRU2017 若手プログラム</a>に参加．最優秀研究計画賞受賞．</li>
								<li>2017年 7月 Amazon Robotics Challenge 2017にチームメンバとして参加．Stow Task 3rd Place.</li>
								<li>2016年 7月 Amazon Picking Challenge 2016にチームメンバとして参加．</li>
								<li>2016年 1月 NEXT COMMUNICATION AWARD 2015 サービスアイデア部門に作品提出．準グランプリ受賞．</li>
							</ul>

						</div>
					</section>



				<!-- 所持資格・スキル -->
					<section id="skill" class="seven">
						<div class="container">

							<header>
								<h2>所持資格・スキル</h2>
							</header>

							<h3>国家資格</h3>
							<p>（IPA情報処理技術者試験）</p>
							<ul>
								<li>2010年11月 ITパスポート試験（PBT） 合格</li>
								<li>2013年 5月 基本情報技術者試験 合格</li>
								<li>2016年 6月 応用情報技術者試験 合格</li>
							</ul>
							<p>（自動車運転免許）</p>
							<ul>
								<li>2013年 3月 第一種普通自動車免許（現・準中型5t限定） 取得</li>
								<li>2015年 3月 普通自動二輪免許 取得</li>
								<li>2017年 7月 第一種準中型自動車免許（限定解除） 取得</li>
								<li>2018年 3月 大型自動二輪免許 取得</li>
							</ul>
							<p>（その他）</p>
							<ul>
								<li>2014年 1月 第三級アマチュア無線技士 取得</li>
								<li>2019年11月 第三級陸上特殊無線技士 取得</li>
							</ul>

							<h3>語学系</h3>
							<ul>
								<li>2018年12月 TOEIC L&R (IP) 650</li>
							</ul>

							<h3>民間資格・その他</h3>
							<ul>
								<li>2016年 1月 CG-ARTS協会 画像処理エンジニア検定ベーシック 合格</li>
								<li>2016年 1月 CG-ARTS協会 画像処理エンジニア検定エキスパート 合格</li>
								<li>2016年 2月 （一社）音楽電子事業協会 MIDI検定3級 合格</li>
								<li>2016年10月 三菱電機（株） MELFAロボットスクール Fコース 修了</li>
								<li>2020年 3月 （一社）日本ディープラーニング協会（JDLA） Deep Learning for ENGINEER 2020 #1 合格</li> 
								<li>その他民間資格約10件</li>
							</ul>

						</div>
					</section>


			</div>

		<!-- Footer -->
			<div id="footer">

				<!-- Copyright -->
					<ul class="copyright">
						<li>&copy; Ryosuke Araki. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a>. Modified by ryorsk.</li>
					</ul>

			</div>

		<!-- Modal Windows -->
			<div id="graspability" class="modal js-modal">
				<div class="modal_bg js-modal-close"></div>
				<div class="modal_content">
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
					<img src="images/grasping_det.jpg" alt="" />
					<h3>Graspabilityを導入したDCNNによる物体把持位置検出</h3>
					<p>産業用ロボットや生活支援ロボットでピッキングを実現するには，ロボットに搭載されているカメラセンサを用いて物体の画像を撮影し，
						その物体の最適な把持位置を検出する必要があります．
						本研究では，Deep Convolutional Neural Networkの学習過程に把持可能性 (Graspability) を導入することで，高精度な物体の把持位置検出法を実現します．
						実験の結果，従来の物体把持位置検出法と比べて約22.8%性能が向上していることが確認できます．
						また，本手法は10fpsで把持位置を検出できます．</p>
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
				</div>
			</div>

			<div id="objectness" class="modal js-modal">
				<div class="modal_bg js-modal-close"></div>
				<div class="modal_content">
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
					<img src="images/objectness-ssd.jpg" alt="" />
					<h3>Objectnessを導入したSSDによる未学習物体の検出</h3>
					<p>物流倉庫や生活空間では日々新しい物体が追加されるため，この環境で稼働するロボットには，追加されたばかりの未学習物体を検出する機能が望まれます．
						本研究では，物体検出アルゴリズムのSingle Shot Multibox Detector (SSD) に「物体らしさ」を表すObjectnessを導入します．
						これにより，学習データに含まれていないアイテム (未知クラスアイテム) に対して，物体であるか，そうでないかの認識が可能になります．
						Amazon Robotics Challenge (ARC) のために作成したデータセットを用いた評価実験により，
						提案手法は49.59%の未知クラスアイテムを検出できることを確認しました．</p>
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
				</div>
			</div>

			<div id="mt_dssd" class="modal js-modal">
				<div class="modal_bg js-modal-close"></div>
				<div class="modal_content">
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
					<img src="images/mt-dssd.jpg" alt="" />
					<h3>MT-DSSDを用いた物体と物体把持位置の同時検出</h3>
					<p>物流倉庫や生活空間で稼働するロボットには，多品種の物体から特定の物体を探して把持する機能が望まれます．
						本研究では，深層学習を用いた物体検出アルゴリズムであるDeconvolutional Single Shot Detector (DSSD) をベースとして，
						物体検出・セマンティックセグメンテーション・物体把持位置検出の3タスクを同時実行するMulti-task DSSD (MT-DSSD) を提案します．
						DSSDから得られるマルチスケールの特徴マップに対して，従来の物体検出だけでなくセマンティックセグメンテーションを行うサブネットワーク (ブランチ)
						および把持位置検出と当該把持位置の把持しやすさを推定するブランチを追加しました．
						また，3タスクを同時に学習することで，各タスクの精度が向上する効果も確認できました．
						提案手法を実際のロボットシステムに組み込んだ結果，Amazon Robotics Challenge競技用アイテムのほとんどを8割以上の精度で把持できました．
						MT-DSSDの推論に必要な時間は300ミリ秒程度であり，把持動作のボトルネックになることはありません．</p>
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
				</div>
			</div>

			<div id="occlusion_det" class="modal js-modal">
				<div class="modal_bg js-modal-close"></div>
				<div class="modal_content">
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
					<img src="images/occlusion_det.jpg" alt="" />
					<h3>[共同研究] 多品種ばら積みピッキングにおける物体間の上下関係の予測</h3>
					<p>多品種ばら積みのピッキングでは，オクルージョンが発生した物体を把持する場合，他物体が上に存在するため把持に失敗することがあります．
						これを解決するには，物体間の上下関係を予測して把持戦略を決定する必要があります．
						本研究では，ばら積みされた物体間の上下関係を獲得するための新たなデータセットである ARC Multi-task Dataset を提案し，
						物体間の上下関係の予測を行います．
						また，この問題を解くためのアイデアとして，物体領域と重なった部分をセグメンテーションするOccluded Area Mask R-CNNと，
						オクルージョンが発生している領域を補完するInterpolation Mask R-CNNも同時に提案しています．
						オクルージョン領域を予測して，ある物体がどの物体に隠されているかを判断し，重なり順序を推定できます．</p>
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
				</div>
			</div>

			<div id="gan_da" class="modal js-modal">
				<div class="modal_bg js-modal-close"></div>
				<div class="modal_content">
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
					<img src="images/gan-da.jpg" alt="" />
					<h3>[共同研究] 脳腫瘍検出に向けたPGGANによるMR画像の水増し</h3>
					<p>（このプロジェクトは，MIRU2017若手プログラムにて新規研究プロポーザルを行い，最優秀研究計画賞を受賞したものです．
						その後も実際に研究を続け，査読付きジャーナル「IEEE Access」へ採録されるに至りました．）</p>
					<p>医用画像処理の分野では，一般的な画像処理分野よりもデータ量が遥かに少ないことから，機械学習を応用させることが困難です．
						これを解決するには，学習用画像にアフィン変換やノイズ付与などを行いデータを水増し (Data Augmentation) することが定石です．
						本研究では，ノイズから画像を生成するGenerative Adversarial Networks (GANs) を用いてリアルな脳MRIを生成し，水増しに用います．
						提案手法では，非常に高解像度な画像を生成するProgressive growing of GANs (PGGAN) を用いて得られた脳MRIに対して，
						画像の精錬を行うSimGANやM-UNITを用いて，よりリアルな脳MRIを生成します．
						この生成画像をオリジナルの学習データに混ぜ込んで学習することで，脳腫瘍検出精度が向上しました．</p>
					<p class="right"><a class="js-modal-close" href="">閉じる</a></p>
				</div>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script src="assets/js/modalwindow.js"></script>

	</body>
</html>
